# -*- coding: utf-8 -*-
"""AIM_KNN_(Hybrid_models).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tCWyUFdOq_bmexP4slffeh3Fbk5ZH747
"""

from google.colab import drive
drive.mount('/content/drive')

"""## **KNN**"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import seaborn as sns
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, classification_report
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
# %matplotlib inline
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.tokenize import RegexpTokenizer
nltk.download('punkt')
nltk.download('stopwords')
import re
from PIL import Image, ImageDraw
from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator
import warnings, gc

def clean_html(text):
  html = re.compile('<.*?>')#regex
  return html.sub(r'',text)
def email_address(text):
  email = re.compile(r'[\w\.-]+@[\w\.-]+')
  return email.sub(r'',text)
def remove_(tweet):
  tweet = re.sub('([_]+)', "", tweet)
  return tweet
def remove_digits(text):
    pattern = r'[^a-zA-z.,!?/:;\"\'\s]'
    #when the ^ is on the inside of []; we are matching any character that is not included in this expression within the []
    return re.sub(pattern, '', text)
def remove_links(tweet):
  '''Takes a string and removes web links from it'''
  tweet = re.sub(r'http\S+', '', tweet) # remove http links
  tweet = re.sub(r'bit.ly/\S+', '', tweet) # remove bitly links
  tweet = tweet.strip('[link]') # remove [links]
  return tweet
def clean_html(text):
  html = re.compile('<.*?>')#regex
  return html.sub(r'',text)
def remove_special_characters(text):
    # define the pattern to keep
    pat = r'[^a-zA-z0-9.,!?/:;\"\'\s]'
    return re.sub(pat, '', text)
def removeStopWords(str):
#select english stopwords
  cachedStopWords = set(stopwords.words("english"))
#add custom words
  cachedStopWords.update(('and','I','A','http','And','So','arnt','This','When','It','many','Many','so','cant','Yes','yes','No','no','These','these','mailto','regards','ayanna','like','email'))
#remove stop words
  new_str = ' '.join([word for word in str.split() if word not in cachedStopWords])
  return new_str
def non_ascii(s):
  return "".join(i for i in s if ord(i)<128)
def punct(text):
  token=RegexpTokenizer(r'\w+')#regex
  text = token.tokenize(text)
  text= " ".join(text)
  return text
def non_ascii(s):
  return "".join(i for i in s if ord(i)<128)
def lower(text):
  return text.lower()

df = pd.read_csv('/content/drive/MyDrive/Symptom2Disease.csv')
df.drop('Unnamed: 0',axis=1,inplace=True)
df = df.sample(frac=0.5).reset_index(drop=True)
df.head()

df.shape

df.info()

df = df.drop_duplicates()
df.shape

size = (500, 500)
mask = Image.new('RGB', size, (255, 255, 255))
draw = ImageDraw.Draw(mask)
draw.ellipse((0, 0, size[0], size[1]), fill=(0, 0, 0))
mask.save("circle_mask.png")

label_counts = df.groupby('label').size().reset_index(name='count')
sorted_labels = label_counts.sort_values(by='count', ascending=False)
sorted_df = df.merge(sorted_labels, on='label')
text = ' '.join(sorted_df['text'])
mask = np.array(Image.open("circle_mask.png"))
wc = WordCloud(background_color='white', contour_color='white', contour_width=1, mask=mask)
wordcloud = wc.generate(text)
plt.figure(figsize=(8, 8))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()

max_len = 150
colors = sns.color_palette('husl', len(df['label'].unique()))
plt.figure(figsize=(16,10))
fig = sns.countplot(x='label',data=df, palette=colors)
plt.xticks(rotation=90,)
plt.tight_layout()
for p in fig.patches:
    fig.annotate(format(p.get_height(), '.0f'),
                   (p.get_x() + p.get_width() / 2., p.get_height()),
                   ha = 'center', va = 'center',
                   xytext = (0, 9),
                   textcoords = 'offset points')
plt.show(fig)
plt.close('all')
del fig

def clean_text(df,col):
    df[col] = df[col].apply(func=clean_html)
    df[col] = df[col].apply(func=email_address)
    df[col] = df[col].apply(func=remove_)
    df[col] = df[col].apply(func=remove_digits)
    df[col] = df[col].apply(func=remove_links)
    df[col] = df[col].apply(func=remove_special_characters)
    df[col] = df[col].apply(func=removeStopWords)
    df[col] = df[col].apply(func=non_ascii)
    df[col] = df[col].apply(func=punct)
    df[col] = df[col].apply(func=lower)
    return df

preprocessed_df = clean_text(df,'text')
preprocessed_df.head()

from collections import  Counter
corpus = []
for x in df['text'].str.split():
    corpus.extend(x)
counter=Counter(corpus)
most=counter.most_common()
print(most[0:10])

first_n = 25
x, y= [], []
for word,count in most[:first_n]:
        x.append(word)
        y.append(count)
plt.figure(figsize=(10, 10))
sns.barplot(x=y,y=x)
print(f"{first_n} most frequently occurring words in symptom descriptions")

nltk.download('stopwords')
stop=set(stopwords.words('english'))

stop_words = set(stopwords.words('english'))
def preprocess_text(text):
    # Tokenizacja
    words = word_tokenize(text.lower())
    # Removing stopwords and non-alphabetic characters
    words = [word for word in words if word.isalpha() and word not in stop_words]
    return ' '.join(words)

preprocessed_symptoms = preprocessed_df['text'].apply(preprocess_text)
preprocessed_symptoms.head()

tfidf_vectorizer = TfidfVectorizer(max_features=1500)
tfidf_features = tfidf_vectorizer.fit_transform(preprocessed_symptoms).toarray()

X_train, X_test, y_train, y_test = train_test_split(tfidf_features, df['label'] , test_size=0.2, random_state=42)

knn_classifier = KNeighborsClassifier(n_neighbors=5)
knn_classifier.fit(X_train, y_train)

predictions = knn_classifier.predict(X_test)

accuracy = accuracy_score(y_test, predictions)
print(f'Accuracy: {accuracy:.2f}')
print(classification_report(y_test, predictions))

conf_matrix = confusion_matrix(y_test, predictions)
plt.figure(figsize=(10, 8))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=df['label'].unique(), yticklabels=df['label'].unique())
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

symptom = "Swelling in legs, inflamed calves"
preprocessed_symptom = preprocess_text(symptom)
symptom_tfidf = tfidf_vectorizer.transform([preprocessed_symptom])
predicted_disease = knn_classifier.predict(symptom_tfidf)
print(f'Predicted Disease: {predicted_disease[0]}')

symptom = "Swollen lymph nodes, red spots"
preprocessed_symptom = preprocess_text(symptom)
symptom_tfidf = tfidf_vectorizer.transform([preprocessed_symptom])
predicted_disease = knn_classifier.predict(symptom_tfidf)
print(f'Predicted Disease: {predicted_disease[0]}')

"""## **Voting Classifier**"""

# Import necessary libraries
from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import pandas as pd
import seaborn as sns
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import VotingClassifier, RandomForestClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize, RegexpTokenizer
import re
from PIL import Image, ImageDraw
from wordcloud import WordCloud
import warnings, gc

# Download necessary NLTK data
nltk.download('punkt')
nltk.download('stopwords')

# Preprocessing functions
def clean_html(text):
    html = re.compile('<.*?>')  # regex
    return html.sub(r'', text)

def email_address(text):
    email = re.compile(r'[\w\.-]+@[\w\.-]+')
    return email.sub(r'', text)

def remove_(tweet):
    return re.sub('([_]+)', "", tweet)

def remove_digits(text):
    pattern = r'[^a-zA-Z.,!?/:;\"\'\s]'
    return re.sub(pattern, '', text)

def remove_links(tweet):
    tweet = re.sub(r'http\S+', '', tweet)  # remove http links
    tweet = re.sub(r'bit.ly/\S+', '', tweet)  # remove bitly links
    return tweet.strip('[link]')

def remove_special_characters(text):
    pat = r'[^a-zA-Z0-9.,!?/:;\"\'\s]'
    return re.sub(pat, '', text)

def removeStopWords(str):
    cachedStopWords = set(stopwords.words("english"))
    cachedStopWords.update(('and','I','A','http','And','So','arnt','This','When','It','many','Many','so','cant','Yes','yes','No','no','These','these','mailto','regards','ayanna','like','email'))
    return ' '.join([word for word in str.split() if word not in cachedStopWords])

def non_ascii(s):
    return "".join(i for i in s if ord(i) < 128)

def punct(text):
    token = RegexpTokenizer(r'\w+')  # regex
    text = token.tokenize(text)
    return " ".join(text)

def lower(text):
    return text.lower()

# Load and preprocess the data
df = pd.read_csv('/content/drive/MyDrive/Symptom2Disease.csv')
df.drop('Unnamed: 0', axis=1, inplace=True)
df = df.sample(frac=0.5).reset_index(drop=True)

df = df.drop_duplicates()

# Generate a WordCloud
size = (500, 500)
mask = Image.new('RGB', size, (255, 255, 255))
draw = ImageDraw.Draw(mask)
draw.ellipse((0, 0, size[0], size[1]), fill=(0, 0, 0))
mask.save("circle_mask.png")

label_counts = df.groupby('label').size().reset_index(name='count')
sorted_labels = label_counts.sort_values(by='count', ascending=False)
sorted_df = df.merge(sorted_labels, on='label')
text = ' '.join(sorted_df['text'])
mask = np.array(Image.open("circle_mask.png"))
wc = WordCloud(background_color='white', contour_color='white', contour_width=1, mask=mask)
wordcloud = wc.generate(text)
plt.figure(figsize=(8, 8))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()

# Plot label distribution
colors = sns.color_palette('husl', len(df['label'].unique()))
plt.figure(figsize=(16,10))
fig = sns.countplot(x='label',data=df, palette=colors)
plt.xticks(rotation=90,)
plt.tight_layout()
for p in fig.patches:
    fig.annotate(format(p.get_height(), '.0f'),
                   (p.get_x() + p.get_width() / 2., p.get_height()),
                   ha = 'center', va = 'center',
                   xytext = (0, 9),
                   textcoords = 'offset points')
plt.show(fig)
plt.close('all')
del fig

# Clean the text data
def clean_text(df, col):
    df[col] = df[col].apply(func=clean_html)
    df[col] = df[col].apply(func=email_address)
    df[col] = df[col].apply(func=remove_)
    df[col] = df[col].apply(func=remove_digits)
    df[col] = df[col].apply(func=remove_links)
    df[col] = df[col].apply(func=remove_special_characters)
    df[col] = df[col].apply(func=removeStopWords)
    df[col] = df[col].apply(func=non_ascii)
    df[col] = df[col].apply(func=punct)
    df[col] = df[col].apply(func=lower)
    return df

preprocessed_df = clean_text(df, 'text')

# Analyze word frequency
from collections import Counter
corpus = []
for x in df['text'].str.split():
    corpus.extend(x)
counter = Counter(corpus)
most = counter.most_common()
print(most[0:10])

first_n = 25
x, y = [], []
for word, count in most[:first_n]:
        x.append(word)
        y.append(count)
plt.figure(figsize=(10, 10))
sns.barplot(x=y, y=x)
print(f"{first_n} most frequently occurring words in symptom descriptions")

# Preprocess text data for modeling
stop_words = set(stopwords.words('english'))

def preprocess_text(text):
    words = word_tokenize(text.lower())
    words = [word for word in words if word.isalpha() and word not in stop_words]
    return ' '.join(words)

preprocessed_symptoms = preprocessed_df['text'].apply(preprocess_text)

# TF-IDF Vectorization
tfidf_vectorizer = TfidfVectorizer(max_features=1500)
tfidf_features = tfidf_vectorizer.fit_transform(preprocessed_symptoms).toarray()

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(tfidf_features, df['label'], test_size=0.2, random_state=42)

# Define individual models
knn_classifier = KNeighborsClassifier(n_neighbors=5)
rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)
svm_classifier = SVC(probability=True, random_state=42)
nb_classifier = MultinomialNB()

# Create a Voting Classifier (ensemble model)
voting_classifier = VotingClassifier(
    estimators=[
        ('knn', knn_classifier),
        ('rf', rf_classifier),
        ('svm', svm_classifier),
        ('nb', nb_classifier)
    ],
    voting='soft'  # 'soft' uses predicted probabilities, 'hard' uses predicted class labels
)

# Train the Voting Classifier
voting_classifier.fit(X_train, y_train)

# Make predictions and evaluate accuracy
predictions = voting_classifier.predict(X_test)
accuracy = accuracy_score(y_test, predictions)
print(f'Ensemble Model Accuracy: {accuracy:.2f}')
print(classification_report(y_test, predictions))

# Confusion Matrix
conf_matrix = confusion_matrix(y_test, predictions)
plt.figure(figsize=(10, 8))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=df['label'].unique(), yticklabels=df['label'].unique())
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix - Ensemble Model')
plt.show()

# Predict using the ensemble model
symptom = "Swelling in legs, inflamed calves"
preprocessed_symptom = preprocess_text(symptom)
symptom_tfidf = tfidf_vectorizer.transform([preprocessed_symptom])

# Convert to dense format for prediction
predicted_disease = voting_classifier.predict(symptom_tfidf.toarray())
print(f'Predicted Disease: {predicted_disease[0]}')

symptom = "Swollen lymph nodes, red spots"
preprocessed_symptom = preprocess_text(symptom)
symptom_tfidf = tfidf_vectorizer.transform([preprocessed_symptom])

# Convert to dense format for prediction
predicted_disease = voting_classifier.predict(symptom_tfidf.toarray())
print(f'Predicted Disease: {predicted_disease[0]}')



"""## **Stacking Model Approach**"""

# Import necessary libraries
from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import pandas as pd
import seaborn as sns
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import StackingClassifier, RandomForestClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize, RegexpTokenizer
import re
from PIL import Image, ImageDraw
from wordcloud import WordCloud
import warnings, gc

# Download necessary NLTK data
nltk.download('punkt')
nltk.download('stopwords')

# Preprocessing functions
def clean_html(text):
    html = re.compile('<.*?>')  # regex
    return html.sub(r'', text)

def email_address(text):
    email = re.compile(r'[\w\.-]+@[\w\.-]+')
    return email.sub(r'', text)

def remove_(tweet):
    return re.sub('([_]+)', "", tweet)

def remove_digits(text):
    pattern = r'[^a-zA-Z.,!?/:;\"\'\s]'
    return re.sub(pattern, '', text)

def remove_links(tweet):
    tweet = re.sub(r'http\S+', '', tweet)  # remove http links
    tweet = re.sub(r'bit.ly/\S+', '', tweet)  # remove bitly links
    return tweet.strip('[link]')

def remove_special_characters(text):
    pat = r'[^a-zA-Z0-9.,!?/:;\"\'\s]'
    return re.sub(pat, '', text)

def removeStopWords(str):
    cachedStopWords = set(stopwords.words("english"))
    cachedStopWords.update(('and','I','A','http','And','So','arnt','This','When','It','many','Many','so','cant','Yes','yes','No','no','These','these','mailto','regards','ayanna','like','email'))
    return ' '.join([word for word in str.split() if word not in cachedStopWords])

def non_ascii(s):
    return "".join(i for i in s if ord(i) < 128)

def punct(text):
    token = RegexpTokenizer(r'\w+')  # regex
    text = token.tokenize(text)
    return " ".join(text)

def lower(text):
    return text.lower()

# Load and preprocess the data
df = pd.read_csv('/content/drive/MyDrive/Symptom2Disease.csv')
df.drop('Unnamed: 0', axis=1, inplace=True)
df = df.sample(frac=0.5).reset_index(drop=True)

df = df.drop_duplicates()

# Clean the text data
def clean_text(df, col):
    df[col] = df[col].apply(func=clean_html)
    df[col] = df[col].apply(func=email_address)
    df[col] = df[col].apply(func=remove_)
    df[col] = df[col].apply(func=remove_digits)
    df[col] = df[col].apply(func=remove_links)
    df[col] = df[col].apply(func=remove_special_characters)
    df[col] = df[col].apply(func=removeStopWords)
    df[col] = df[col].apply(func=non_ascii)
    df[col] = df[col].apply(func=punct)
    df[col] = df[col].apply(func=lower)
    return df

preprocessed_df = clean_text(df, 'text')

# Preprocess text data for modeling
stop_words = set(stopwords.words('english'))

def preprocess_text(text):
    words = word_tokenize(text.lower())
    words = [word for word in words if word.isalpha() and word not in stop_words]
    return ' '.join(words)

preprocessed_symptoms = preprocessed_df['text'].apply(preprocess_text)

# TF-IDF Vectorization
tfidf_vectorizer = TfidfVectorizer(max_features=1500)
tfidf_features = tfidf_vectorizer.fit_transform(preprocessed_symptoms).toarray()

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(tfidf_features, df['label'], test_size=0.2, random_state=42)

# Define base models
knn_classifier = KNeighborsClassifier(n_neighbors=5)
rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)
svm_classifier = SVC(probability=True, random_state=42)
nb_classifier = MultinomialNB()

# Define the meta-classifier (Logistic Regression)
meta_classifier = LogisticRegression()

# Create a Stacking Classifier (ensemble model)
stacking_classifier = StackingClassifier(
    estimators=[
        ('knn', knn_classifier),
        ('rf', rf_classifier),
        ('svm', svm_classifier),
        ('nb', nb_classifier)
    ],
    final_estimator=meta_classifier,
    stack_method='predict_proba'  # Use predicted probabilities for stacking
)

# Train the Stacking Classifier
stacking_classifier.fit(X_train, y_train)

# Make predictions and evaluate accuracy
predictions = stacking_classifier.predict(X_test)
accuracy = accuracy_score(y_test, predictions)
print(f'Stacking Model Accuracy: {accuracy:.2f}')
print(classification_report(y_test, predictions))

# Confusion Matrix
conf_matrix = confusion_matrix(y_test, predictions)
plt.figure(figsize=(10, 8))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=df['label'].unique(), yticklabels=df['label'].unique())
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix - Stacking Model')
plt.show()

# Predict using the Stacking Classifier
symptom = "Swelling in legs, inflamed calves"
preprocessed_symptom = preprocess_text(symptom)
symptom_tfidf = tfidf_vectorizer.transform([preprocessed_symptom])

# Convert to dense format for prediction
symptom_tfidf_dense = symptom_tfidf.toarray()

predicted_disease = stacking_classifier.predict(symptom_tfidf_dense)
print(f'Predicted Disease: {predicted_disease[0]}')

symptom = "Swollen lymph nodes, red spots"
preprocessed_symptom = preprocess_text(symptom)
symptom_tfidf = tfidf_vectorizer.transform([preprocessed_symptom])

# Convert to dense format for prediction
symptom_tfidf_dense = symptom_tfidf.toarray()

predicted_disease = stacking_classifier.predict(symptom_tfidf_dense)
print(f'Predicted Disease: {predicted_disease[0]}')
