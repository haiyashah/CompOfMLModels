# -*- coding: utf-8 -*-
"""AIM_NaiveBayes_Hybrid_Models.ipynb"""

import numpy as np
import pandas as pd
import seaborn as sns
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.ensemble import VotingClassifier, StackingClassifier, RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
import matplotlib.pyplot as plt
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize, RegexpTokenizer
import re
from PIL import Image, ImageDraw
from wordcloud import WordCloud
import warnings, gc

nltk.download('punkt')
nltk.download('stopwords')

# Preprocessing functions
def clean_html(text):
    html = re.compile('<.*?>')
    return html.sub(r'', text)

def email_address(text):
    email = re.compile(r'[\w\.-]+@[\w\.-]+')
    return email.sub(r'', text)

def remove_(tweet):
    return re.sub('([_]+)', "", tweet)

def remove_digits(text):
    pattern = r'[^a-zA-Z.,!?/:;\"\'\s]'
    return re.sub(pattern, '', text)

def remove_links(tweet):
    tweet = re.sub(r'http\S+', '', tweet)
    tweet = re.sub(r'bit.ly/\S+', '', tweet)
    return tweet.strip('[link]')

def remove_special_characters(text):
    pat = r'[^a-zA-Z0-9.,!?/:;\"\'\s]'
    return re.sub(pat, '', text)

def removeStopWords(str):
    cachedStopWords = set(stopwords.words("english"))
    cachedStopWords.update(('and','I','A','http','And','So','arnt','This','When','It','many','Many','so','cant','Yes','yes','No','no','These','these','mailto','regards','ayanna','like','email'))
    return ' '.join([word for word in str.split() if word not in cachedStopWords])

def non_ascii(s):
    return "".join(i for i in s if ord(i) < 128)

def punct(text):
    token = RegexpTokenizer(r'\w+')
    text = token.tokenize(text)
    return " ".join(text)

def lower(text):
    return text.lower()

# Load and preprocess the data
df = pd.read_csv('/content/Symptom2Disease.csv')
df.drop('Unnamed: 0', axis=1, inplace=True)
df = df.sample(frac=0.5).reset_index(drop=True)
df = df.drop_duplicates()

# Generate a WordCloud
size = (500, 500)
mask = Image.new('RGB', size, (255, 255, 255))
draw = ImageDraw.Draw(mask)
draw.ellipse((0, 0, size[0], size[1]), fill=(0, 0, 0))
mask.save("circle_mask.png")

label_counts = df.groupby('label').size().reset_index(name='count')
sorted_labels = label_counts.sort_values(by='count', ascending=False)
sorted_df = df.merge(sorted_labels, on='label')
text = ' '.join(sorted_df['text'])
mask = np.array(Image.open("circle_mask.png"))
wc = WordCloud(background_color='white', contour_color='white', contour_width=1, mask=mask)
wordcloud = wc.generate(text)
plt.figure(figsize=(8, 8))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()

# Plot label distribution
colors = sns.color_palette('husl', len(df['label'].unique()))
plt.figure(figsize=(16,10))
fig = sns.countplot(x='label',data=df, palette=colors)
plt.xticks(rotation=90,)
plt.tight_layout()
for p in fig.patches:
    fig.annotate(format(p.get_height(), '.0f'),
                   (p.get_x() + p.get_width() / 2., p.get_height()),
                   ha = 'center', va = 'center',
                   xytext = (0, 9),
                   textcoords = 'offset points')
plt.show(fig)
plt.close('all')
del fig

# Clean the text data
def clean_text(df, col):
    df[col] = df[col].apply(func=clean_html)
    df[col] = df[col].apply(func=email_address)
    df[col] = df[col].apply(func=remove_)
    df[col] = df[col].apply(func=remove_digits)
    df[col] = df[col].apply(func=remove_links)
    df[col] = df[col].apply(func=remove_special_characters)
    df[col] = df[col].apply(func=removeStopWords)
    df[col] = df[col].apply(func=non_ascii)
    df[col] = df[col].apply(func=punct)
    df[col] = df[col].apply(func=lower)
    return df

preprocessed_df = clean_text(df, 'text')

# Analyze word frequency
from collections import Counter
corpus = []
for x in df['text'].str.split():
    corpus.extend(x)
counter = Counter(corpus)
most = counter.most_common()
print(most[0:10])

first_n = 25
x, y = [], []
for word, count in most[:first_n]:
        x.append(word)
        y.append(count)
plt.figure(figsize=(10, 10))
sns.barplot(x=y, y=x)
print(f"{first_n} most frequently occurring words in symptom descriptions")

# Preprocess text data for modeling
stop_words = set(stopwords.words('english'))

def preprocess_text(text):
    words = word_tokenize(text.lower())
    words = [word for word in words if word.isalpha() and word not in stop_words]
    return ' '.join(words)

preprocessed_symptoms = preprocessed_df['text'].apply(preprocess_text)

# TF-IDF Vectorization
tfidf_vectorizer = TfidfVectorizer(max_features=1500)
tfidf_features = tfidf_vectorizer.fit_transform(preprocessed_symptoms).toarray()

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(tfidf_features, df['label'], test_size=0.2, random_state=42)

# Naive Bayes Model
nb_classifier = MultinomialNB()
nb_classifier.fit(X_train, y_train)

# Predictions and evaluation
predictions = nb_classifier.predict(X_test)
accuracy = accuracy_score(y_test, predictions)
print(f'Naive Bayes Accuracy: {accuracy:.2f}')
print(classification_report(y_test, predictions))

# Confusion Matrix
conf_matrix = confusion_matrix(y_test, predictions)
plt.figure(figsize=(10, 8))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=df['label'].unique(), yticklabels=df['label'].unique())
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix - Naive Bayes')
plt.show()

# Hybrid Model 1: Voting Classifier
voting_classifier = VotingClassifier(
    estimators=[
        ('nb', nb_classifier),
        ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),
        ('svm', SVC(probability=True, random_state=42)),
        ('knn', KNeighborsClassifier(n_neighbors=5))
    ],
    voting='soft'  # 'soft' uses predicted probabilities, 'hard' uses predicted class labels
)

voting_classifier.fit(X_train, y_train)
predictions = voting_classifier.predict(X_test)
accuracy = accuracy_score(y_test, predictions)
print(f'Voting Classifier Accuracy: {accuracy:.2f}')
print(classification_report(y_test, predictions))

conf_matrix = confusion_matrix(y_test, predictions)
plt.figure(figsize=(10, 8))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=df['label'].unique(), yticklabels=df['label'].unique())
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix - Voting Classifier')
plt.show()

# Hybrid Model 2: Stacking Classifier
stacking_classifier = StackingClassifier(
    estimators=[
        ('nb', nb_classifier),
        ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),
        ('svm', SVC(probability=True, random_state=42)),
        ('knn', KNeighborsClassifier(n_neighbors=5))
    ],
    final_estimator=LogisticRegression(random_state=42)
)

stacking_classifier.fit(X_train, y_train)
predictions = stacking_classifier.predict(X_test)
accuracy = accuracy_score(y_test, predictions)
print(f'Stacking Classifier Accuracy: {accuracy:.2f}')
print(classification_report(y_test, predictions))

conf_matrix = confusion_matrix(y_test, predictions)
plt.figure(figsize=(10, 8))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=df['label'].unique(), yticklabels=df['label'].unique())
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix - Stacking Classifier')
plt.show
